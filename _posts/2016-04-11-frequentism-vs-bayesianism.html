---
layout: post
title: "Frequentism vs Bayesianism"
date: 2016-04-11
comments: true
archive: true
notes: true
tags: notes
excerpt: Thoughts regarding the subtle differences between Frequentism and Bayesianism.
---

<p>
The disagreement between <b>Frequentism</b> and <b>Bayesianism</b> concerns the
definition of probability.
</p>

<p>
<b>Frequentism</b> interprets probability as the limiting case of repeated
measurements.
</p>

<p>
probabilites = fundamentally related frequency of any given value
</p>

<p>
<b>Bayesianism</b> interprets probability as degrees of certainty about
 statements.
</p>

<p>
probability = fundamentally related to our knowledge about an event
</p>

<p>
probability = statement of my knowledge of what the measurement result
will be
</p>

<p>
<b>Example:</b> Given a measurements of a photon flux \(F\) from a given star
 estimate the true flux of the star.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Different views</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">&#xa0;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left"><b>Frequentist view:</b></td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Since by definition the probability of the true flux of a star is a fixed value then it is meaningless for the frequentist to talk about the probability. Talking about frequency distribution for a fixed value is nonsense.</td>
</tr>

<tr>
<td class="org-left"><b>Bayesian view:</b></td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Claims to measure the flux \(F\) of a star with some probability \(P(F)\). Although the probability can be estimated from frequencies of repeated experiments it is not fundamental.</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Can meaningfully talk about the probability that the true flux of a star lies in a given range</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Probability codifies our knowledge of the value based on prior information and/or available data</td>
</tr>

<tr>
<td class="org-left">Philosophy difference leads to</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">different approaches to statistical analysis of data</td>
</tr>
</tbody>
</table>


<p>
<code>An example illustrating the different approaches:</code>
</p>

<p>
<span class="underline">Notation:</span>
</p>

<p>
\(N\) = measurements
</p>

<p>
\(i^{th}\) = observation
</p>

<p>
\(F_{i}\) = photon flux
</p>

<p>
\(e_{i}\) = error related to \(F_{i}\)
</p>


<p>
Assumptions regarding \(e_{i}\): \(e_{i}\; \sim\; \cal{N}(\mu,
\sigma^{2})\)
</p>

<p>
<b>Frequentist:</b>
</p>

<p>
\(e_{i} = \frac{\sigma}{std}\) of the results of a single measurement
event w.r.t. the limit of the repetitions of that event.
</p>

<p>
<b>Bayesian:</b>
</p>

<p>
\(e_{i} = \frac{\sigma}{std}\) of (Gaussian) probability distribution
describing our knowledge of that particular measurement given its
observed value.
</p>

<p>
Given \(D = \{F_{i}, e_{i}\}\) find the best estimate of the true
F<sub>true</sub>?
</p>

<p>
Since \(N \in \mathbb{N}\) thus a Poisson distribution is a good
approximation to the measurement process.
</p>

<p>
\(F \;\sim\; \text{Pois}(\lambda_{i}), i = 1,\dots,n\)
\(F \;\sim\; \text{Pois}(\lambda)\)
</p>

<p>
\(\{N_{i}, e_{i}\}\) is estimated from Poisson statistics using the
standard square-root rule.
</p>

<p>
The true \(F_{true}\) flux is already known: \(F_{true} = 1000\), numbers
of photons measured in 1 sec.
</p>


<ol class="org-ol">
<li><p>
<b>Frequentist approach:</b>
</p>

<p>
Start classic maximum likelihood!  Given \(D_{i} = \{F_{i},
   e_{i}\}\), compute the probability distribution of the measurement
given the true \(F_{true}\) flux, given also the assumption of
Gaussian errors.
</p>

<p>
\[P(D_{i} | F_{true}) = \frac{1}{\sqrt{2\pi
   e^{2}_{i}}}\text{exp}[-\frac{(F_{i} - F_{true})^{2}}{2e^{2}_{i}}]\]
</p>

<p>
Construct likelihood function = product of probabilities for each
data point.
</p>

<p>
\[\cal{L}(D | F_{true}) = \prod_{i=1}^{N} P(D | F_{true})\]
</p>

<p>
Since \(\cal{L}\) becomes very small to avoid underflow/overflow
errors we compute the log-likelihood.
</p>

<p>
\[\text{log}(\cal{L}) = -\frac{1}{2}\sum^{N}_{i=1}[\text{log}(2\pi
   e^{2}_{i}) + \frac{(F_{i} - F_{true})^2}{e^{2}_{i}}]\]
</p>

<p>
Determine \(F_{true}\) susch that the likelihood is maximized.
</p>

<p>
Computed analytically by setting
\([\frac{d\text{log}\cal{L}}{dF_{true}} = 0]\)
</p>

<p>
\(F_{est.} = \frac{\sum w_{i}F_{i}}{\sum w_{i}};\;\; w_{i} =
   \frac{1}{e^{2}_{i}}\;\; \text{if}\;\forall e_{i}\; \text{being
   equal}\)
</p>

<p>
this reduces to \(F_{est.} = \frac{1}{N}\sum_{i=1}^{N} F_{i}
   \rightarrow\; \text{simply the mean of the observed data when
   errors are equal}\)
</p>

<p>
What is the error of \(F_{est.}\)? Identifying the error in the two
different approaches.
</p>

<p>
Accomplished by <span class="underline">fitting a Gaussian</span> approximation <span class="underline">to the
likelihood</span> curve <span class="underline">at the maximum</span>.
</p>

<p>
For the simple case we can solve it analytically.
</p>

<p>
\(\sigma_{est.} = (\sum_{i=1}^{N} w_{i})^{-\frac{1}{2}}\rightarrow\;
   \text{std. of Gaussian approximation}\)
</p></li>
</ol>

<p>
<b>2. Bayesian approach:</b>
</p>

<p>
Begins and ends with probabilities. We want to compute our
knowledge of the parameters in question,
i.e. \(P(F_{true}|D)\rightarrow\;\text{Bayesian} \neq P(D |
   F_{true})\rightarrow\;\text{Frequentist}\)
</p>

<p>
Formulation of the problem is fundamentally contrary to the
frequentist philosophy.
</p>

<p>
It says that the <span class="underline">probabilities have no meaning for model
parameters</span> like \(F_{true}\).
</p>

<p>
To compute $P(F<sub>true</sub> | D) they apply Bayes Rule.$
</p>

<p>
\[P(F_{true} | D) = \frac{P(D | F_{true}) P(F_{true})}{P(D)}\]
</p>

<p>
What is controversial is not the Bayes law but instead the Bayesian
interpretation of probability by the term \(P(F_{true} | D)\).
</p>

<ul class="org-ul">
<li>\(P(F_{true} | D):\) posterior/probability of the model parameters
given data. Result we want to compute.</li>

<li>\(P(D | F_{true}):\) likelihood, proportional to \(\cal{L}(D |
   F_{true})\) in the frequentist approach above.</li>

<li>\(P(F_{true}):\) model prior, encodes what we knew about the mdoel
prior to the application of the data \(D\).</li>

<li>\(P(D):\) data probability, in practice amounts to a normalization
term</li>
</ul>


<p>
Setting \(P(F_{true})\;\propto\) 1 (a flat prior), we find \(P(F_{true} |
D)\propto\;\cal{L}(D | F_{true})\). For a flat prior the <b>Bayesian</b> and
the <b>Frequentist</b> become similar to each other.
</p>

<p>
The prior \(P(F_{true})\) allows inclusion of other information into the
computation, useful in cases of combining multiple measurement
strategies.
</p>

<p>
One of the most controversial pieces of <b>Bayesian</b> analysis is to
specify the necessity of a prior.
</p>

<p>
<b>Frequentist</b> will point out that the prior is problematic when no
 true prior information is available. In many situations a trully
 non-informative prior doesn't exist.
</p>

<p>
<b>Frequentists</b> say the choice of prior necessarily biases your results
 and therefore has no place in statistical data analysis.
</p>

<p>
<b>Bayesian</b> would say that frequentism can be viewed as a simple case
 of the <b>Bayesian</b> approach for some (implicit) choice of the prior:
</p>

<p>
<b>Bayesians</b> argue that it would be better to make this implicit choice
 explicit, even if the choice might include some subjectivity.
</p>

<p>
How <b>Bayesian</b> results are computed in practice?
</p>

<p>
For a one parameter problem compute the posterior probability
\(P(F_{true} | D)\) as a function of \(F_{true}\).
</p>

<p>
In other words compute the distribution reflecting our knowledge of
the parameter \(F_{true}\).
</p>

<p>
The direct approach becomes increasingly intractable as the dimension
of the model grows.
</p>

<p>
<b>Bayesian</b> calculations often depend on sampling methods such as
 Markov Chain Monte Carlo (MCMC).
</p>

<p>
The <b>Goal</b> is to <span class="underline">generate a set of points draw from the posterior
probability distribution</span> and use them to determine the answer we seek.
</p>

<p>
In pure <b>Bayesianism</b> the answer to a question is not a single number
with error bars; the answer is the posterior distribution over the
model parameters.
</p>

<p>
<b>Exploring a more sophisticated model: Adding a Dimension</b>
</p>

<p>
Assume our observing object has some stochastic variation, i.e. it
varies with time.
</p>

<p>
Propose a simple 2-parameter \(\cal{N}(\mu, \sigma), \theta = [\mu,
\sigma]\) of the variability intrinsic to the object.
</p>

<p>
<b>Model:</b> \(F_{true}\;\sim\;\frac{1}{\sqrt{2\pi
 \sigma^{2}}}\text{exp}[\frac{(F - \mu)^{2}}{2\sigma^{2}}]\)
</p>


<ol class="org-ol">
<li><p>
<b>Frequentist approach:</b>
</p>

<p>
\[\cal{L}(D | \theta) =
   \prod_{i=1}^{N}\frac{1}{\sqrt{2\pi(\sigma^{2} +
   e^{2}_{i})}}\text{exp}[-\frac{-(F_{i} - \mu)^{2}}{2(\sigma^{2} +
   e^{2}_{i})}]\]
</p>

<p>
<b>Likelihood</b> is the convolution of the intrinsic distribution with
the error distribution.
</p>

<p>
Analytically maximize the above likelihood to find the best
estimate for \(\mu\):
</p>

<p>
\(\mu_{est.} = \frac{\sum w_{i}F_{i}}{\sum w_{i}}\;; w_{i} =
   \frac{1}{\sigma^{2} + e^{2}_{i}}\)
</p>

<p>
Here we have <b>a problem</b>: The optimal value of \(\mu\) depends on the
optimal value of \(\sigma\).
</p>

<p>
Results are correlated \(\rightarrow\) no longer possible to use
analytic methods to arrive at the <b>Frequentist</b> result.
</p>

<p>
But we can use numerical optimization techniques to determine the
maximum likelihood value.
</p>

<p>
Maximum likelihood gives best estimate of the parameters \(\mu\) and
\(\sigma\) governing our model. This is only half the answer.
</p></li>
</ol>

<p>
<span class="underline">We need to compute Error Bars on \(\mu\) and \(\sigma\)</span>.
</p>

<p>
Several approaches to determine errors in a frequentist
approach/paradigm.
</p>

<ol class="org-ol">
<li>Fit a normal approximation to the maximum likelihood and report the
covariance matrix (do thsi numerically rather than analytically)</li>

<li>Alternatively, compute statistics \(\chi^{2}\) and
\(\chi^{2}_{\text{dof}}\) to and standard tests to determine
confidence limits, which also depends on strong assumptions about
the Gaussianity of the likelihood.</li>

<li>Alternatively, use randomized sampling aproaches such as
<i>Jackknife</i> or <i>Bootstrap</i>, which maximize the likelihood
randomized samples of the input data in order to explore the degree
of certainty in the result.</li>
</ol>

<p>
In bootstrapping or sampling techniques there is a potential for
errors to be correlated or even non-Gaussian, neither of which is
reflected by simply finding the mean and std. of each model parameter.
</p>

<ol class="org-ol">
<li><p>
<b>Bayesian approach:</b>
</p>

<p>
Almost exactly the same as it was in the previous problem.
</p>

<p>
The vast majority of commonly applied frequentist techniques make
the explicit or implicit assumption of Gaussianity of the
distribution.
</p>

<p>
Bayesian approaches generally don't require such assumptions
</p>

<p>
There are good arguements that a prior on \(\sigma\) subtley or
subjectively biases the calculation in this case.
</p></li>
</ol>

<p>
<b>Conclusion:</b>
</p>

<p>
<b>Bayesianism</b> and <b>Frequentism</b> are fundamentally different approaches
 to simple problems which can yield similar or even identical results.
</p>

<p>
<b>Differences:</b>
</p>

<p>
<b>Frequentism</b> considers probabilities related to frequencies of real
 or hypothetical events.
</p>

<p>
<b>Bayesianism</b> considers probabilities as measurements of degrees of
 knowledge
</p>

<p>
<b>Frequentist</b> analyses generally proceeds through the use of point
 estimates and maximum likelihood
</p>

<p>
<b>Baysian</b> analyses generally compute the posterior either directly or
 through some version of MCMC sampling
</p>

<p>
In simple problems, the two approaches yield similar results. As data
and models grow in complexity the two approaches can diverge
greatly. This is clea in 2 situations.
</p>

<ol class="org-ol">
<li>The handling of nuisance parameters</li>
<li>The subtle (often overlooked) difference between <b>frequentist
confidence intervals</b> and <b>Bayesian credible regions</b></li>
</ol>

<p>
Credits to <a href="http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/">Jake VanderPlas</a>
</p>
