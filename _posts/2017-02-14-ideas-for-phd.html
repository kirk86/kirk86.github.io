layout: post
title: "Ideas regarding PhD topic"
date: 2017-02-14
comments: true
archive: true
excerpt: Thoughts and notes regarding PhD topics

<p>
<b>Ideas:</b>
</p>

<p>
Kernels run out of memory while NN's are compact function classes
providing a trade off between storage vs training time computation.
</p>

<p>
Exploit the trades of both of the methodls and combine them for
nonparametric statistical test, generative modes, message passing,
bandit algorithms and other things that need good statistical analysis
and flexible models.
</p>

<p>
<code>Problem which still remains to be solved, is how to incorporate model</code>
<code>decompositions efficiently into deep learning?</code>
</p>


<p>
<code>deep learning + spectral methods ==&gt; How to combine them?</code>
</p>

<p>
<code>This can be done e.g. using some of the objective functions for from</code>
<code>graphical models .e.g. Conditional Random Fields, Structured loss,</code>
<code>anything similar</code>
</p>

<p>
<b>Differences between graphical models and deep learning:</b>
</p>

<ul class="org-ul">
<li><b>graphical models</b> are good if you've got a lot of variables and
want to know how they depend on each other. Explains a lot about
clustering, topic models, Bayesian nonparametrics, causality and
message passing</li>
</ul>


<ul class="org-ul">
<li><b>deep learning</b> is about understanding how to use them efficiently
and which are the limitations. <i>Statistical learning theory</i> in this
case is necessary to prove theorems about whether your algorithm
works or not. You want to have a guarantee for what you're doing
won't go wrong but you don't really want to use the theoremsfor
parameter tuning.</li>
</ul>


<p>
<b>LSTM's</b> are latent variable auto-regressive models with some fine
 tuning to deal with vanishing gradients
</p>

<p>
Adverserial Environments hard to handle
</p>


<p>
<b>The Master Algorithm by P.Domingos:</b>
</p>

<p>
At the time of writing it has been identified that there are 5
different tribes, schools/paradigms regarding machine learning related
to the way that each school or technique uses they preferred
methodology or algorithm inside the machine learning community.
</p>

<p>
How do computers discover new knowledge?
</p>

<ol class="org-ol">
<li>Fill in gaps in existing knowledge</li>
<li>Emulate the brain</li>
<li>Simulate evolution</li>
<li>Systematically reduce uncertainty</li>
<li>Notice similarities between old and new</li>
</ol>


<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Tribe</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">Origins</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">Master Algorithm</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">People</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Symbolists</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Logic, philosophy</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Inverse deduction</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Tom Mitchel, Steve Muggleton, Ross Quinlan</td>
</tr>

<tr>
<td class="org-left">Connectionists</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Neuroscience</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Backpropagation</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">LeCun, Hinton, Bengio</td>
</tr>

<tr>
<td class="org-left">Evolutionaries</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Evolutionary Biology</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Genetic programming</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">John Koza, John Holland, Hod Lipson</td>
</tr>

<tr>
<td class="org-left">Bayesians</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Statistics</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Probabilistic inference</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">David Heckerman, Judea Pearl, Michael Jordan</td>
</tr>

<tr>
<td class="org-left">Analogizers</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Psychology</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Kernel machines</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Peter Hart, V.Vapnik, Douglas Hofstadter</td>
</tr>
</tbody>
</table>


<p>
Putting pieces together:
</p>

<ul class="org-ul">
<li>Representation
<ul class="org-ul">
<li>Probabilistic logic (e.g. Markov logic networks)</li>
<li>Weighted formulas &#x2013;&gt; Distriubtion over states</li>
</ul></li>

<li>Evaluation
<ul class="org-ul">
<li>Posterior probability</li>
<li>User defined objective function</li>
</ul></li>

<li>Optimization
<ul class="org-ul">
<li>Formula discovery: Genetic programming</li>
<li>Weight learning: Backpropagation</li>
</ul></li>

<li>Towards a universal learner
<ul class="org-ul">
<li>New ideas and tribes are needed ==&gt; ?</li>
</ul></li>
</ul>

<p>
Grand unifying theory =&gt; unify all 5 learning tribes
</p>

<p>
Unifying representations =&gt; starting from theorist and Bayesians
(logic and graphical models =&gt; has been done =&gt; Markov Logic Networks)
</p>

<ol class="org-ol">
<li>Start with a FOL rule if&#x2026;then&#x2026;</li>
<li>Give each rule a weight depending whether you believe it or not</li>
<li>Evaluation function. Find in the hypotheses space the candidate
that maximizes or minimizes my evaluation function. In this case
that's just the posterior that Bayesians use. It shouldn't be part
of the algorithm. It should be provided by the user. The objective
function to optimize should be given by the user.</li>
<li>How do we find the model to optimize the algorithm. When you have
your formulas you have to come up with weights for optimizing those
formulas i.e. Backprop.</li>
</ol>

<p>
Different projects:
</p>

<p>
Project 1: Methods for Semi-supervised Learning and Active Labeling
How can we exploit unlabeled data for a supervised learning problem
and how can we identify the most informative subset of examples to be
annotated by an expert?
</p>

<p>
Project 2: Methods for Robust Feature Learning How can we learn robust
features that remain maximally predictive even if the distribution of
test data is very different from the distribution of training data?
</p>

<p>
Project 3: Calibrated Uncertainty Estimation How can we provide
reliable confidence intervals for deep neural network predictions?
</p>
