<div class="HTML">
<p>
&#x2014;
layout: post
title: "Ideas regarding PhD topic"
date: 2016-04-02
comments: true
archive: true
notes: true
tags: notes
excerpt: Thoughts and notes regarding PhD topics
&#x2014;
</p>

</div>

<p>
<b>Ideas:</b>
</p>

<p>
Kernels run out of memory while NN's are compact function classes
providing a trade off between storage vs training time computation.
</p>

<p>
Exploit the trades of both of the methodls and combine them for
nonparametric statistical test, generative modes, message passing,
bandit algorithms and other things that need good statistical analysis
and flexible models.
</p>

<p>
<code>Problem which still remains to be solved, is how to incorporate model</code>
<code>decompositions efficiently into deep learning?</code>
</p>


<p>
<code>deep learning + spectral methods ==&gt; How to combine them?</code>
</p>

<p>
<code>This can be done e.g. using some of the objective functions for from</code>
<code>graphical models .e.g. Conditional Random Fields, Structured loss,</code>
<code>anything similar</code>
</p>

<p>
<b>Differences between graphical models and deep learning:</b>
</p>

<ul class="org-ul">
<li><b>graphical models</b> are good if you've got a lot of variables and
want to know how they depend on each other. Explains a lot about
clustering, topic models, Bayesian nonparametrics, causality and
message passing</li>
</ul>


<ul class="org-ul">
<li><b>deep learning</b> is about understanding how to use them efficiently
and which are the limitations. <i>Statistical learning theory</i> in this
case is necessary to prove theorems about whether your algorithm
works or not. You want to have a guarantee for what you're doing
won't go wrong but you don't really want to use the theoremsfor
parameter tuning.</li>
</ul>


<p>
<b>LSTM's</b> are latent variable auto-regressive models with some fine
 tuning to deal with vanishing gradients
</p>

<p>
Adverserial Environments hard to handle
</p>


<p>
<b>The Master Algorithm by P.Domingos:</b>
</p>

<p>
At the time of writing it has been identified that there are 5
different tribes, schools/paradigms regarding machine learning related
to the way that each school or technique uses they preferred
methodology or algorithm inside the machine learning community.
</p>

<p>
How do computers discover new knowledge?
</p>

<ol class="org-ol">
<li>Fill in gaps in existing knowledge</li>
<li>Emulate the brain</li>
<li>Simulate evolution</li>
<li>Systematically reduce uncertainty</li>
<li>Notice similarities between old and new</li>
</ol>


<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Tribe</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">Origins</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">Master Algorithm</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">People</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Symbolists</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Logic, philosophy</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Inverse deduction</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Tom Mitchel, Steve Muggleton, Ross Quinlan</td>
</tr>

<tr>
<td class="org-left">Connectionists</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Neuroscience</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Backpropagation</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">LeCun, Hinton, Bengio</td>
</tr>

<tr>
<td class="org-left">Evolutionaries</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Evolutionary Biology</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Genetic programming</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">John Koza, John Holland, Hod Lipson</td>
</tr>

<tr>
<td class="org-left">Bayesians</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Statistics</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Probabilistic inference</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">David Heckerman, Judea Pearl, Michael Jordan</td>
</tr>

<tr>
<td class="org-left">Analogizers</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Psychology</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Kernel machines</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Peter Hart, V.Vapnik, Douglas Hofstadter</td>
</tr>
</tbody>
</table>


<p>
Putting pieces together:
</p>

<ul class="org-ul">
<li>Representation
<ul class="org-ul">
<li>Probabilistic logic (e.g. Markov logic networks)</li>
<li>Weighted formulas &#x2013;&gt; Distriubtion over states</li>
</ul></li>

<li>Evaluation
<ul class="org-ul">
<li>Posterior probability</li>
<li>User defined objective function</li>
</ul></li>

<li>Optimization
<ul class="org-ul">
<li>Formula discovery: Genetic programming</li>
<li>Weight learning: Backpropagation</li>
</ul></li>

<li>Towards a universal learner
<ul class="org-ul">
<li>New ideas and tribes are needed ==&gt; ?</li>
</ul></li>
</ul>

<p>
Grand unifying theory =&gt; unify all 5 learning tribes
</p>

<p>
Unifying representations =&gt; starting from theorist and Bayesians
(logic and graphical models =&gt; has been done =&gt; Markov Logic Networks)
</p>

<ol class="org-ol">
<li>Start with a FOL rule if&#x2026;then&#x2026;</li>
<li>Give each rule a weight depending whether you believe it or not</li>
<li>Evaluation function. Find in the hypotheses space the candidate
that maximizes or minimizes my evaluation function. In this case
that's just the posterior that Bayesians use. It shouldn't be part
of the algorithm. It should be provided by the user. The objective
function to optimize should be given by the user.</li>
<li>How do we find the model to optimize the algorithm. When you have
your formulas you have to come up with weights for optimizing those
formulas i.e. Backprop.</li>
</ol>

<p>
Different projects:
</p>

<p>
Project 1: Methods for Semi-supervised Learning and Active Labeling
How can we exploit unlabeled data for a supervised learning problem
and how can we identify the most informative subset of examples to be
annotated by an expert?
</p>

<p>
Project 2: Methods for Robust Feature Learning How can we learn robust
features that remain maximally predictive even if the distribution of
test data is very different from the distribution of training data?
</p>

<p>
Project 3: Calibrated Uncertainty Estimation How can we provide
reliable confidence intervals for deep neural network predictions?
</p>

<p>
Project 4: Methods for Multimodal Learning and Sensor Fusion How can
we combine multiple sources of information to improve prediction
accuracy?
</p>

<p>
Project 5: Combining Generative Probabilistic Models with Deep
Learning How can we use probabilistic, possibly causal, graphical
models, or complex simulators, to improve the accuracy of a
classifier?
</p>

<p>
Project 6: Model Compression and Distillation How can we maximally
compress the amount of bits necessary to store and execute a deep
neural network while maintaining high accuracy?
</p>

<p>
Project 7: Reinforcement Learning and Planning How can we use RL to
plan the actions of e.g. a car in traffic, given sensory information
of its surroundings?
</p>

<p>
Project 8: Learning color-invariant bases Can robust, universally
applicable color-invariants be learned in the lower layers of CNN’s
that facilitate image classification?
</p>

<p>
Project 9: Learning to follow objects over multiple cameras Can we
learn the characteristics of objects as observed from multiple
camera’s images without a priori knowledge on the camera’s properties,
their frames or the objects?
</p>

<p>
Project 10: Learning from images near the boundary of a class How can
we learn from adversarial examples or hard positive/negative examples
and how can we make classifiers perform robustly when confronted with
adversarial examples?
</p>
